# -*- coding: utf-8 -*-
"""bert_text_summerization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HCQ0wtZLH_efgaEXr8JBizGHyuDG4cLR

# Text summarization with BERT
"""

import json
import re
import os
import string

import numpy as np
import tensorflow as tf
from transformers import BertConfig, TFBertModel, BertTokenizer

max_len = 384

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
save_path = 'bert_base_uncased'

if not os.path.exists(save_path): os.makedirs(save_path)

tokenizer.save_pretrained(save_path)

# a faster method to create tokenizer

from tokenizers import BertWordPieceTokenizer

tokenizer = BertWordPieceTokenizer(save_path + '/vocab.txt', lowercase=True)

""" Loading data """

# train_data_url = 'https://github.com/rajpurkar/SQuAD-explorer/blob/master/dataset/train-v1.1.json'
# test_data_url = 'https://github.com/rajpurkar/SQuAD-explorer/blob/master/dataset/dev-v1.1.json'

# train_path = tf.keras.utils.get_file('train.json', train_data_url)
# test_path = tf.keras.utils.get_file('test.json', test_data_url)

train_path = 'data/datasets/train.json'
test_path = 'data/datasets/test.json'

# json.load(open(test_path))

class SquadExamples():
    def __init__(self, question, context, start_char_index, answer_text, all_answers):
        self.question = question
        self.context = context
        self.start_char_index = start_char_index
        self.answer_text = answer_text
        self.all_answers = all_answers
        self.skip = False
    
    def preprocess(self):
        question = self.question
        context = self.context
        start_char_index = self.start_char_index
        answer_text = self.answer_text

        question = ''.join(str(question).split())
        context = ''.join(str(context).split())
        answer = ''.join(str(answer_text).split())

        # find end char index in context
        end_char_index = start_char_index + len(answer)
        if end_char_index >= len(context):
            self.skip = True
            return
        
        # create a mask list
        is_char_in_ans = [0] * len(context)
        for index in range(start_char_index, end_char_index):
            is_char_in_ans[index] = 1
        
        tokenized_context = tokenizer.encode(context)

        ans_token_index = []
        for index, (start, end) in enumerate(tokenized_context.offsets):
            if sum(is_char_in_ans[start:end]) > 0:
                ans_token_index.append(index)
        
        if len(ans_token_index) == 0:
            self.skip = True
            return
        
        start_token_index = ans_token_index[0]
        end_token_index = ans_token_index[-1]

        tokenized_question = tokenizer.encode(question)

        # create inputs
        input_ids = tokenized_context.ids + tokenized_question.ids[1:]
        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])
        attention_mask = [1] * len(input_ids)

        # padding & skip rules
        padding_length = max_len - len(input_ids)
        if padding_length > 0:
            input_ids += ([0] * padding_length)
            attention_mask += ([0] * padding_length)
            token_type_ids += ([0] * padding_length)
        elif padding_length < 0:
            self.skip = True
            return
        
        self.start_token_index = start_token_index
        self.end_token_index = end_token_index
        self.context_token2char = tokenized_context.offsets
        self.input_ids = input_ids
        self.token_type_ids = token_type_ids
        self.attention_mask = attention_mask

with open(train_path) as f:
    raw_train_data = json.load(f)

with open(test_path) as f:
    raw_test_data = json.load(f)

def create_squad_examples(raw_data):
    # too many for loops; to be optimized
    squad_examples = []

    # data cleaning
    for item in raw_data['data']:
        paras = item['paragraphs']
        for para in paras:
            context = para['context']
            qas = para['qas']
            for qa in qas:
                question = qa['question']
                if len(qa['answers']) > 0:
                    answer_text = qa['answers'][0]['text']
                    start_char_index = qa['answers'][0]['answer_start']
                    all_answers = [_['text'] for _ in qa['answers']]
                elif len(qa['plausible_answers']) > 0:
                    answer_text = qa['plausible_answers'][0]['text']
                    start_char_index = qa['plausible_answers'][0]['answer_start']
                    all_answers = [_['text'] for _ in qa['plausible_answers']]
                else:
                    answer_text = ''
                    start_char_index = len(context)
                    all_answers = ''

                # try:
                #     answer_text = qa['answers'][0]['text']
                # except:
                #     print(qa)
                #     break

                squad_example = SquadExamples(
                    question, context, start_char_index, answer_text, all_answers
                )
                squad_example.preprocess()
                squad_examples.append(squad_example)
    
    return squad_examples

def create_inputs_targets(squad_examples):
    dataset_dict = {
        'start_token_index': [], 
        'end_token_index': [], 
        'input_ids': [], 
        'token_type_ids': [], 
        'attention_mask': [],
    }

    for item in squad_examples:
        if item.skip == False:
            for key in dataset_dict:
                dataset_dict[key].append(getattr(item, key))
    
    for key in dataset_dict:
        dataset_dict[key] = np.array(dataset_dict[key])
    
    x = [
        dataset_dict['input_ids'], 
        dataset_dict['token_type_ids'], 
        dataset_dict['attention_mask'], 
    ]
    y = [
        dataset_dict['start_token_index'], 
        dataset_dict['end_token_index'], 
    ]

    return x, y

train_squad_examples = create_squad_examples(raw_train_data)
test_squad_examples = create_squad_examples(raw_test_data)

x_train, y_train = create_inputs_targets(train_squad_examples)
x_test, y_test = create_inputs_targets(test_squad_examples)

# print(len(train_squad_examples), len(test_squad_examples))

""" Create model """

def create_model():
    bert = TFBertModel.from_pretrained('bert-base-uncased')

    input_ids = tf.keras.layers.Input(shape=(max_len, ), dtype=tf.int32)
    token_type_ids = tf.keras.layers.Input(shape=(max_len, ), dtype=tf.int32)
    attention_mask = tf.keras.layers.Input(shape=(max_len, ), dtype=tf.int32)

    embedding = bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]

    start_logit = tf.keras.layers.Dense(1, name='start_logit', use_bias=False)(embedding)
    end_logit = tf.keras.layers.Dense(1, name='end_logit', use_bias=False)(embedding)

    start_logit = tf.keras.layers.Flatten()(start_logit)
    end_logit = tf.keras.layers.Flatten()(end_logit)

    start_prob = tf.keras.layers.Activation(tf.keras.activations.softmax)(start_logit)
    end_prob = tf.keras.layers.Activation(tf.keras.activations.softmax)(end_logit)

    model = tf.keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask], 
        outputs=[start_prob, end_prob]
    )

    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)
    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
    
    model.compile(optimizer=optimizer, loss=[loss, loss])

    return model

strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
    model = create_model()
    model.summary()

def normalized_text(text):
    text = text.lower()

    # remove punctuation
    exclude = set(string.punctuation)
    text = ''.join(ch for ch in text if ch not in exclude)

    # remove articles
    regex = re.compile(r'\b(a|an|the)\b', re.UNICODE)
    text = re.sub(regex, ' ', text)

    # remove extra blank
    text = ' '.join(text.split())

    return text

class ExactMatch(tf.keras.callbacks.Callback):
    def __init__(self, x_test, y_test):
        self.x_test = x_test
        self.y_test = y_test
    
    def on_epoch_end(self, epoch, logs=None):
        pred_start, pred_end = self.model.predict(self.x_test)
        count = 0
        test_example_no_skip = [_ for _ in test_squad_examples if _.skip == False]

        for index, (start, end) in enumerate(zip(pred_start, pred_end)):
            # transfer prediction into text
            squad_example = test_example_no_skip[index]
            offsets = squad_example.context_token2char
            start = np.argmax(start)
            end = np.argmax(end)

            if start >= len(offsets):
                continue
            
            pred_char_start = offsets[start][0]

            if end < len(offsets):
                pred_char_end = offsets[end][1]
                pred_ans = squad_example.context[pred_char_start:pred_char_end]
            else:
                pred_ans = squad_example[pred_char_start:]
            
            normalized_pred_ans = normalized_text(pred_ans)
            normalized_true_ans = [normalized_text(_) for _ in squad_example.all_answers]

            if normalized_pred_ans in normalized_true_ans:
                count += 1
        
        acc = count / len(self.y_test[0])
        print(f'\nepoch={epoch+1}, exact match score={acc:.2f}')

exact_match_callback = ExactMatch(x_test, y_test)

model.fit(
    x_train, 
    y_train, 
    epochs=1, 
    verbose=1, 
    batch_size=16, 
    callbacks=[exact_match_callback]
)

model_save_path = save_path + '/models'

if not os.path.exists(model_save_path): os.makedirs(model_save_path)

model.save(model_save_path + '/bert_text_summ_model.h5')